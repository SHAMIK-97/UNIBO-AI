{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using several classifiers and tuning parameters - Parameters grid\n",
    "[From official `scikit-learn` documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)\n",
    "\n",
    "Adapted by Claudio Sartori\n",
    "\n",
    "Example of usage of the ***model selection*** features of `scikit-learn` and comparison of several classification methods.\n",
    "1. import a sample dataset \n",
    "1. split the dataset into two parts: train and test\n",
    "    - the *train* part will be used for training and validation (i.e. for *development*)\n",
    "    - the *test* part will be used for test (i.e. for *evaluation*)\n",
    "    - the fraction of test data will be _ts_ (a value of your choice between 0.2 and 0.5)\n",
    "1. the function `GridSearchCV` iterates a cross validation experiment to train and test a model with different combinations of paramater values\n",
    "    - for each parameter we set a list of values to test, the function will generate all the combinations\n",
    "    - we choose a *score function* which will be used for the optimization\n",
    "        - e.g. `accuracy_score`, `precision_score`, `cohen_kappa_score`, `f1_score`, see this [page](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for reference\n",
    "    - the output is a dictionary containing \n",
    "        - the set of parameters which maximize the score \n",
    "        - the test scores\n",
    "1. prepare the parameters for the grid\n",
    "    - it is a list of dictionaries\n",
    "1. set the parameters by cross validation and the *score functions* to choose from\n",
    "1. Loop on scores and, for each score, loop on the model labels (see details below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
      "@author: scikit-learn.org and Claudio Sartori\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "@author: scikit-learn.org and Claudio Sartori\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # uncomment this line to suppress warnings\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "print(__doc__) # print information included in the triple quotes at the beginning\n",
    "\n",
    "# Loading a standard dataset\n",
    "#dataset = datasets.load_digits()\n",
    "# dataset = datasets.fetch_olivetti_faces() # 40 classes!\n",
    "# dataset = datasets.fetch_covtype()        # 581012 examples\t 54 features \n",
    "# dataset = datasets.load_iris()    # 150 examples -- 4 features -- 3 classes\n",
    "dataset = datasets.load_wine()      # 178 examples -- 13 features -- 3 classes\n",
    "# dataset = datasets.load_breast_cancer() # binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prepare the environment\n",
    "The `dataset` module contains, among others, a few sample datasets.\n",
    "\n",
    "See this [page](http://scikit-learn.org/stable/datasets/index.html) for reference\n",
    "\n",
    "Prepare the data and the target in X and y. Set `ts`. Set the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    178 examples -- 13 features -- 3 classes\n"
     ]
    }
   ],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into the train and test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 124 examples\n"
     ]
    }
   ],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is intended to ease the remainder of the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_lbls = [\n",
    "             'dt', \n",
    "             'nb', \n",
    "#              'lp', \n",
    "#              'svc', \n",
    "#              'knn',\n",
    "            ]\n",
    "\n",
    "# Set the parameters to be explored by the grid for each classifier\n",
    "tuned_param_dt = [{'max_depth': list(range(1,20))}]\n",
    "tuned_param_nb = [{'var_smoothing': [10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-07, 1e-8, 1e-9, 1e-10]}]\n",
    "tuned_param_lp = [{'early_stopping': [True]}]\n",
    "tuned_param_svc = [{'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                    'C': [1, 10, 100, 1000],\n",
    "                    },\n",
    "                    {'kernel': ['linear'],\n",
    "                     'C': [1, 10, 100, 1000],                     \n",
    "                    },\n",
    "                   ]\n",
    "tuned_param_knn =[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "\n",
    "# set the models to be fitted specifying name, estimator and parameter structure\n",
    "models = {\n",
    "    'dt': {'name': 'Decision Tree       ',\n",
    "           'estimator': DecisionTreeClassifier(), \n",
    "           'param': tuned_param_dt,\n",
    "          },\n",
    "    'nb': {'name': 'Gaussian Naive Bayes',\n",
    "           'estimator': GaussianNB(),\n",
    "           'param': tuned_param_nb\n",
    "          },\n",
    "    'lp': {'name': 'Linear Perceptron   ',\n",
    "           'estimator': Perceptron(),\n",
    "           'param': tuned_param_lp,\n",
    "          },\n",
    "    'svc':{'name': 'Support Vector      ',\n",
    "           'estimator': SVC(), \n",
    "           'param': tuned_param_svc\n",
    "          },\n",
    "    'knn':{'name': 'K Nearest Neighbor ',\n",
    "           'estimator': KNeighborsClassifier(),\n",
    "           'param': tuned_param_knn\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "# scores to be explored\n",
    "scores = [\n",
    "          'precision', \n",
    "#           'recall',\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The function below groups all the outputs\n",
    "Write a function which has as parameter the fitted model and uses the components of the fitted model to inspect the results of the search with the parameters grid.\n",
    "\n",
    "The components are:<br>\n",
    "`model.best_params_`<br>\n",
    "`model.cv_results_['mean_test_score']`<br>`\n",
    "model.cv_results_['std_test_score']`<br>\n",
    "`model.cv_results_['params']`\n",
    "\n",
    "The classification report is generated by the function imported above from sklearn.metrics, which takes as argument the true and the predicted test labels.\n",
    "\n",
    "The +/- in the results is obtained doubling the `std_test_score`. Mean and standard test scores are computed considering the various results on the cross-validation chunks.\n",
    "\n",
    "The function will be used to print the results for each set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loop on scores and, for each score, loop on the model labels\n",
    "- iterate varying the score function\n",
    "    1. iterate varying the classification model among Decision Tree, Naive Bayes, Linear Perceptron, Support Vector\n",
    "        - activate the *grid search*\n",
    "            1. the resulting model will be the best one according to the current score function\n",
    "        - print the best parameter set and the results for each set of parameters using the above defined function\n",
    "        - print the classification report\n",
    "        - store the `.best score_` in a dictionary for a final report\n",
    "    1. print the final report for the current *score funtion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Decision Tree       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'max_depth': 12}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "0.419 (+/-0.141) for {'max_depth': 1}\n",
      "0.837 (+/-0.097) for {'max_depth': 2}\n",
      "0.868 (+/-0.040) for {'max_depth': 3}\n",
      "0.867 (+/-0.072) for {'max_depth': 4}\n",
      "0.871 (+/-0.069) for {'max_depth': 5}\n",
      "0.863 (+/-0.101) for {'max_depth': 6}\n",
      "0.855 (+/-0.051) for {'max_depth': 7}\n",
      "0.853 (+/-0.080) for {'max_depth': 8}\n",
      "0.876 (+/-0.124) for {'max_depth': 9}\n",
      "0.886 (+/-0.069) for {'max_depth': 10}\n",
      "0.887 (+/-0.067) for {'max_depth': 11}\n",
      "0.888 (+/-0.064) for {'max_depth': 12}\n",
      "0.851 (+/-0.109) for {'max_depth': 13}\n",
      "0.845 (+/-0.118) for {'max_depth': 14}\n",
      "0.875 (+/-0.080) for {'max_depth': 15}\n",
      "0.881 (+/-0.056) for {'max_depth': 16}\n",
      "0.863 (+/-0.083) for {'max_depth': 17}\n",
      "0.861 (+/-0.117) for {'max_depth': 18}\n",
      "0.867 (+/-0.123) for {'max_depth': 19}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93        20\n",
      "           1       0.95      0.91      0.93        22\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.95      0.95      0.95        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Gaussian Naive Bayes\n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'var_smoothing': 1e-06}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "0.481 (+/-0.010) for {'var_smoothing': 10}\n",
      "0.663 (+/-0.296) for {'var_smoothing': 1}\n",
      "0.754 (+/-0.163) for {'var_smoothing': 0.1}\n",
      "0.743 (+/-0.190) for {'var_smoothing': 0.01}\n",
      "0.755 (+/-0.205) for {'var_smoothing': 0.001}\n",
      "0.878 (+/-0.196) for {'var_smoothing': 0.0001}\n",
      "0.949 (+/-0.103) for {'var_smoothing': 1e-05}\n",
      "0.972 (+/-0.080) for {'var_smoothing': 1e-06}\n",
      "0.958 (+/-0.083) for {'var_smoothing': 1e-07}\n",
      "0.958 (+/-0.083) for {'var_smoothing': 1e-08}\n",
      "0.967 (+/-0.062) for {'var_smoothing': 1e-09}\n",
      "0.967 (+/-0.062) for {'var_smoothing': 1e-10}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        20\n",
      "           1       0.95      0.91      0.93        22\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.94      0.95      0.95        54\n",
      "weighted avg       0.94      0.94      0.94        54\n",
      "\n",
      "\n",
      "Summary of results for precision\n",
      "Estimator\n",
      "Decision Tree       \t - score: 88.77%\n",
      "Gaussian Naive Bayes\t - score: 97.21%\n"
     ]
    }
   ],
   "source": [
    "# insert code here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
