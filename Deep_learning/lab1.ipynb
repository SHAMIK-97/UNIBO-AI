{"cells":[{"cell_type":"markdown","metadata":{"id":"Gb47ZppQnigS"},"source":["# Lecture 1 - Introduction to Deep Learning\n","\n","Refer to https://d2l.ai to an (almost) complete review of the topics we will study during the course.\n","\n","We will use a famous library named TensorFlow (which is now available in version 2+) and in particular one of its subpackages, Keras, together with some utilities libraries like: \n","\n","- Matplotlib (for visualization);\n","- Numpy (to work with arrays);\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"xGhcTwPrpDKz"},"source":["## GPU Runtime\n","\n","Neural Network training requires high parallel computation. To permit it while working on Google Colab, you need to activate GPU Runtime, which can be done by clicking: Runtime -> Change Runtime Type -> GPU.\n","\n","Pay attention! If you don't run any code for some minutes, the GPU Runtime will automatically disconnect."]},{"cell_type":"markdown","metadata":{"id":"YxksrMLBpkjs"},"source":["## Importing Libraries\n","\n","As always, the first thing we need to do is importing libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yKvnH5hpjeb"},"outputs":[],"source":["# Utilities\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Deep Learning\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# Check tensorflow version (must be >2!)\n","print(tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"aLOQ5CBxqGWn"},"source":["## Read a dataset\n","\n","To train a Neural Network model, we will need to load in memory a dataset. You can load it in lots of ways, depending on the time of data that you need.\n","\n","For this course, we will use built-in data on keras. In particoular, we are interested in:\n","\n","- Fashion-MNIST: It is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. \n","\n","For this exercises we will download the dataset locally and experiment with it.\n","You can visualize the data [here](https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=fashion_mnist)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_WkD--Bq-WH"},"outputs":[],"source":["# Import keras dataset Fashion Mnist\n","from tensorflow.keras.datasets import fashion_mnist\n","\n","# Load the data\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","# Check data dimensionality\n","print(f\"Training set dimension: Input {x_train.shape}, Output {y_train.shape}\")\n","print(f\"Test set dimension: Input {x_test.shape}, Output {y_test.shape}\")\n","d = x_train.shape[1]"]},{"cell_type":"markdown","metadata":{"id":"l7mgWPc_r4ph"},"source":["We would like our input data to lies in the interval $[0, 1]$. If our data does not lies in this interval, we can transform it as:\n","\n","$$\n","x' = \\frac{x - x_{min}}{x_{max}-x_{min}}\n","$$\n","\n","Where $x_{min} = \\min(x)$, $x_{max} = \\max(x)$. Note that $x'$ always lies in the interval $[0, 1]$."]},{"cell_type":"code","source":["#Normalization function\n","normalize_data = lambda X: ((X - X.min()) / (X.max() - X.min()))"],"metadata":{"id":"YXkYU8Rf4gKx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This operation is called normalization(min-max feature scaling) and allow us to work easily with the data, speeding up the computation and in some case improving even the results of the training."],"metadata":{"id":"lciQgeQa4Mp3"}},{"cell_type":"code","source":["print(x_train[0])"],"metadata":{"id":"PXlCQp59xoZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6ioxwZutblx"},"outputs":[],"source":["print(f\"Input (train) data lies in the interval [{x_train.min()}, {x_train.max()}]\")\n","print(f\"Input (test) data lies in the interval [{x_test.min()}, {x_test.max()}]\")\n","    \n","x_train = normalize_data(x_train)\n","x_test = normalize_data(x_test)\n","\n","# Check the interval after normalization\n","print(\"\\n\")\n","print(f\"Input (train) data lies in the interval [{x_train.min()}, {x_train.max()}]\")\n","print(f\"Input (test) data lies in the interval [{x_test.min()}, {x_test.max()}]\")"]},{"cell_type":"code","source":["print(x_train[0])"],"metadata":{"id":"ddZ-T8yrxxx0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to transform the $y$ into a vector which is easily understandable from the network. In this case we have to one-hot encode the $y$."],"metadata":{"id":"C0pzrVS56D20"}},{"cell_type":"code","source":["print(f\"y[0]: {y_train[0]}\")\n","\n","# One hot encode the output variables (needed to compute the output)\n","y_train = keras.utils.to_categorical(y_train)\n","y_test = keras.utils.to_categorical(y_test)\n","\n","n_classes = len(y_train[0])\n","\n","print(f\"y[0] after the one-hot encoding: {y_train[0]}\")"],"metadata":{"id":"HUDX5aCm6C1V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Ya_bpK_ubIq"},"source":["## Build the first model!\n","\n","Now that our data is ready, we can build our first model. As you studied, a Neural Network model is defined as a sequence of *Layers*, which is obtained by composing an Affine Transformation with a Non-Linear Activation function. \n","\n","The simplest possible layer is the **Dense** layer, which is the fully-connected layer describing the operation $\\sigma(Ax + b)$, where $A, b$ are learnable parameters, $A$ is a full matrix, and $\\sigma$ is the activation function. Since **Dense** layers applies to vectors (not images), we first need to flatten our data. This can be done either via the **Flatten** layer or via the **Reshape** layer. Moreover, every model must begin with an **Input** layer, that describes the type of data our model will expect as input.\n","\n","### Summary\n","- Input: First Layer of the Network.\n","- Flatten: Utility Layer. It is used to flatten 3-dimensional data of the form $(d_1, d_2, c)$ to a 1-dimensional array of length $d_1 * d_2 * c$. \n","- Reshape: Utility Layer. It reshape the input in the way you want, as long as the dimensions match.\n","- Dense: Basic Layer. It computes a generic Linear transform followed by a non-linear activation function.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZF2FlLI3lGT"},"outputs":[],"source":["# Import Layers from Keras\n","from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n","from tensorflow.keras.models import Model"]},{"cell_type":"markdown","source":["The Sequential API\n","\n","The are two main ways to define models in Tensorflow: the Sequential API and the Functional API."],"metadata":{"id":"mIH4rUCQ72dp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eTbjDH_Mvqc"},"outputs":[],"source":["# Sequential API\n","\n","# Define the model\n","from tensorflow.keras.models import Sequential\n","\n","model = Sequential([Flatten(),\n","                    Dense(units=64, activation='relu'),\n","                    Dense(units=10, activation='softmax')])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGyXK_MIuYDf"},"outputs":[],"source":["# Functional API \n","\n","\"\"\"\n","This implementation is equivalent to the implemantation above with the Sequential\n","API, but it is written using Functional API.\n","\"\"\"\n","\n","def get_model(input_shape, output_shape):\n","    d1, d2 = input_shape\n","    # Define the model by concatenating Layers\n","    x = Input(shape=(d1, d2))\n","\n","    #h = Flatten()(x)\n","    h = Reshape((d1*d2,), input_shape=(d1, d2))(x)\n","    h = Dense(units=64, activation='relu')(h)\n","\n","    y = Dense(units=output_shape, activation='softmax')(h)\n","\n","    # Get the model\n","    model = Model(x, y)\n","\n","    # Visualize summary of the newly created Model\n","    model.summary()\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"6XqQTTMO3lGX"},"source":["### Why you should use the functional model?\n","\n","For this small use case both the Sequential and Functional implementations of the model are correct and apparently equivalent. The former is easier to implement, since it is only required to define the ordering of the layers one after the other and Tensorflow will concatenate them to build the Neural Network. The Functional API instead is harder since it requires to define not only the list of the Layers, but also the relationship between them. On the other hand, the Functional API will allow to define architecture with complex relationship between layers (e.g. skip connections), which is impossible while using Sequential API.\n","One example of the use of skip connections are the Residual Networks. Resnet were proposed by [He et al.](https://arxiv.org/pdf/1512.03385.pdf) in 2015 to solve the image classification problem. In ResNets, the information from the initial layers is passed to deeper layers by matrix addition. This operation doesn’t have any additional parameters as the output from the previous layer is added to the layer ahead. A single residual block with skip connection looks like this:\n","\n","![Resnet](https://www.researchgate.net/profile/Olarik-Surinta/publication/330750910/figure/fig1/AS:720960386781185@1548901759330/Illustration-of-the-residual-learning-between-a-plain-network-and-b-a-residual.ppm)"]},{"cell_type":"markdown","metadata":{"id":"tTwY5vGbzWXb"},"source":["## Train the model\n","\n","To train our model, we first need to **compile** it. Compiling a model means defining a loss function and an optimizer. The loss function should quantify the notion of \"distance\" we want to minimize in our training, while the optimizer is the algorithm that minimize it.\n","\n","In symbols, if we define $f(x; \\theta)$ our model, parameterized by $\\theta$, and we define a loss function $J(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\ell(f(x^{(i)}; \\theta), y^{(i)})$, training means finding a set of parameters $\\theta^*$ that solves\n","\n","$$\n","\\theta^* = \\arg\\min_\\theta J(\\theta)\n","$$\n","\n","This is done by initializing our parameters to a random value $\\theta_0$. The default initializer in Keras is Glorot Uniform, but it can be changed. See https://keras.io/api/layers/initializers/ for more informations.\n","Given $\\theta_0$, the Optimizer $g(\\cdot)$ computes a sequence of updating:\n","\n","$$\n","\\theta_{k+1} = \\theta_k + \\alpha_k g(\\theta_k)\n","$$\n","\n","where $\\alpha_k$ is the *learning rate*.\n","\n","Compiling a model means defining the loss function $J(\\theta)$ (or, equivalently, $\\ell(\\cdot, \\cdot)$), and the optimizer $g(\\cdot)$. Other setting will be defined in the compiling phase. We will talk about them later since they are not necessary.\n","\n","The most used optimizer in the literature is Adam, which is a powerful alternative to the Stochastic Gradient Descent (you already saw in other courses). For a deep explanation on optimizers, refer to https://arxiv.org/abs/1606.04838 . Other optimizier can be choose, for example:\n","\n","* SGD -> Stochastic Gradient Descent\n","* RMSprop\n","* Adadelta\n","* Adagrad\n","\n","For an explanation of the different kind of optimizers, refer to the official documentation https://keras.io/api/optimizers/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKrXXctDyHzV"},"outputs":[],"source":["# Compile the model\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.optimizers import Adam\n","\n","model = get_model((d, d), y_train.shape[-1])\n","\n","model.compile(optimizer=Adam(learning_rate=1e-2), loss='categorical_crossentropy', metrics='accuracy')"]},{"cell_type":"markdown","metadata":{"id":"1bHEG1rE3y2j"},"source":["Now that the model is compiled, we can train it. This is done with the *fit* method. To fit the model, we will need some informations.\n","\n","As we said, training works by minimizing $J(\\theta)$ via a sequence of parameters update\n","\n","$$\n","\\theta_{k+1} = \\theta_k + \\alpha_k g(\\theta_k)\n","$$\n","\n","When the optimizer is chosen to be a variant of Stochastic Gradient Descent (SGD) (e.g. Adam), it takes as input a batch of data.\n","\n","Briefly, consider a dataset of $N$ elemets. Define a parameter $m$, named *batch size* and, at each iteration, select a random subset of data from the entire dataset, of $m$ elements. Those subsets are named *batches*. Given a batch of data, the optimizer takes it as input and compute one step of the training algorithm. Then, the used batch is removed from the original dataset.\n","\n","After all the dataset is being processed, we say that an *epoch* as been executed. This process is repeated for a (usually fixed) number of epochs. Both the batch size and the number of epochs is passed as input to the *fit* method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqecLx5cMzos"},"outputs":[],"source":["# Train the model. We save the output of the training in a variable named \"history\".\n","# This variable contains a report of the behavior of the loss during the iterations.\n","\n","history = model.fit(x_train, y_train, batch_size=128, epochs=30, validation_split=0.1)"]},{"cell_type":"markdown","metadata":{"id":"SwCYwczHNK-P"},"source":["As you can see, the accuracy obtained at the end of the training is incredibly high! Let's see if this behavior is kept when it is tested on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LedGDsRkNWyU"},"outputs":[],"source":["# Compute model accuracy on the test set.\n","model.evaluate(x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"cKNky85xNibl"},"source":["It looks like our model performs very good even on new data! Let's see how we can use it to predict a single shape."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWHHthazNhu2"},"outputs":[],"source":["# Get a single digit and the corresponding label from x_test\n","n = 30 # Index of the data\n","\n","x = x_test[n, :, :]\n","y = y_test[n]\n","\n","# Visualize the shape\n","def show(x, y):\n","    if len(x.shape) == 3:\n","        x = x[:, :, 0]\n","    elif len(x.shape) == 4:\n","        x = x[0, :, :, 0]\n","\n","    true_class = np.argmax(y)\n","    \n","    plt.imshow(x)\n","    plt.gray()\n","    plt.title(f\"True Class: {true_class}\")\n","    plt.show()\n","\n","show(x, y)\n","\n","# We can use the network to predict the value for this digit\n","y_predict = model.predict(np.expand_dims(x, 0))\n","shape_predicted = np.argmax(y_predict)\n","\n","print(f\"Predicted Class: {shape_predicted}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqloMhl5PJnM"},"outputs":[],"source":["# We can also use the history (that we saved before) to check the behavior of the training.\n","# history is a Python dictionary that cointains the values of the behavior of the loss\n","# during training (one value for each epoch).\n","def display_history(history):\n","    mse_training = history.history['loss']\n","    acc_training = history.history['accuracy']\n","\n","    mse_val = history.history['val_loss']\n","    acc_val = history.history['val_accuracy']\n","\n","    # Visualize the behavior of the loss\n","    plt.plot(mse_training)\n","    plt.plot(mse_val)\n","    plt.grid()\n","    plt.title('Loss during training')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Training', 'Validation'])\n","    plt.show()\n","\n","    # and of the accuracy\n","    plt.plot(acc_training)\n","    plt.plot(acc_val)\n","    plt.grid()\n","    plt.title('Accuracy during training')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Training', 'Validation'])\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgbCrTJv3lGl"},"outputs":[],"source":["display_history(history)"]},{"cell_type":"markdown","metadata":{"id":"Dqjo48cD3lGm"},"source":["# Exercise:\n","\n","Try to understand how to use the [sparse categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) by the documentation. Is it conveninent in cases like the one we just saw? \n","If we used it, do the model and the pre-processing change? "]},{"cell_type":"markdown","metadata":{"id":"gStxULdlFSoU"},"source":["# Exercise: Callbacks\n","As you probably noticed in Exercise 2, learning rate and number of epochs are particularly interesting hyperparameters.\n","Indeed, to quicken the training one want to choose a large learning rate which, as a consequence, doesn't permit to achieve the same performance that can be achieved by small learning rate, in more epochs.\n","Intuitively, one wants to start the training with a large learning_rate, and then reduce it when the performance doesn't improve anymore.\n","\n","Similarly, if the number of epochs is too low, the training algorithm does not have enough time to converge, while if it is too large, it takes to much time to conclude the training. Thus, we would like to stop the training when the number of epochs is not decreasing anymore.\n","\n","Such operations can be computed by callbacks, that are algorithms automatically executed during training. Even though you can define your own callback, some of them are already implemented in Keras (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback).\n","\n","One of the most common and most used callback is the EarlyStopping, which allow us to set a series of rule to stop the training part if is useless.\n","\n","How can we use it in our model?"]},{"cell_type":"markdown","metadata":{"id":"kHkpOMW5Xm4R"},"source":["# Exercise 3: Hyperparameter tuning\n","\n","Every parameter that is defined by the user (batch size, number of epoch, number of layers, dimension of each layer, loss function, ...) is called *HyperParameter*, differently from the *Parameters*, that are automatically tuned by the optimizer.\n","\n","Arguably the most important step in Neural Network design is to find the correct set of hyperparameters to solve our task. This step is called *Hyperparameters Tuning*. \n","\n","Try to modify the Neural Network architecture, by adding more layers and modifying the dimension of each layer. What can you observe?\n","\n","*Hint: pay attention to the value of the training Accuracy and the distance between Training Accuracy and Testing Accuracy (Underfit - Overfit).*"]},{"cell_type":"markdown","metadata":{"id":"V7g7bzo2RH2e"},"source":["# Appendix: Advanced training monitoring tools"]},{"cell_type":"markdown","metadata":{"id":"JM771pzbRiBi"},"source":["Sometimes it can be useful to access detailed informations about our model and its training. To do that, we can use TensorBoard, an easy tool to monitoring training details and other informations.\n","\n","Running TensorBoard in Colab is non trivial and requires some setup. First of all, we have to lunch the Tensorboard extension."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VBYJ4ooRhch"},"outputs":[],"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"]},{"cell_type":"markdown","metadata":{"id":"EgNzNuOKW2rI"},"source":["Then, we can remove previous logs. Lunch this command any time you need to refresh the TensorBoard log."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWddGIz3U5Ln"},"outputs":[],"source":["# Clear any logs from previous runs\n","!rm -rf ./logs/ \n","\n","# Import libraries\n","import datetime"]},{"cell_type":"markdown","metadata":{"id":"nlyl3qEAW-ni"},"source":["And then we can define the TensorBoard callback to monitoring training. We will see in the following what are callbacks and how can we use them in general. \n","\n","If you lunch training more then once, without removing the TensorBoard logs, all the informations will be saved and you can compare how hyperparameter tuning modified the final result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sgLWYpAS1IC"},"outputs":[],"source":["# Define the directory in where the log will be located and the corresponding Callback.\n","log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","# Now, we can define our model again and train it, specifying the callback.\n","\n","# Define the model\n","from tensorflow.keras.models import Sequential\n","model = Sequential([Flatten(),\n","                    Dense(units=128, activation='relu'),\n","                    Dense(units=10, activation='softmax')])\n","\n","model.compile(optimizer=Adam(learning_rate=1e-2), loss='sparse_categorical_crossentropy', metrics='accuracy')\n","history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1, callbacks=[tensorboard_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kksNRA0KTNWj"},"outputs":[],"source":["# Start TensorBoard\n","%tensorboard --logdir logs/fit"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["V7g7bzo2RH2e"],"name":"Lecture_1_Introduction_to_Deep_Learning_to_fil.ipynb","provenance":[{"file_id":"1TmmCAAVy8CyBhob-ZrNVt3__u5nrWT3q","timestamp":1647352598759}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}