{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_conv.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QX15PRrJq_zX"},"source":["This notebook is meant to introduce convolutional layers, with special emphasis on the relation between the dimension of the input tensor, the kernel size, the stride, the number of filters and the dimension of the output tensor."]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"jjDI4eEQZ9Xc","executionInfo":{"status":"ok","timestamp":1653524565216,"user_tz":-330,"elapsed":3265,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"atmltv8-UZW9","executionInfo":{"status":"ok","timestamp":1653524568359,"user_tz":-330,"elapsed":813,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["from tensorflow.keras.layers import Input, Conv2D, ZeroPadding2D, Dense, Flatten, Layer\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import metrics\n","from tensorflow.keras.datasets import mnist"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDH8iUaxrvZB"},"source":["We run the example over the mnist data set. Keras provides a very friendly access to several renowed databases, comprising mnist, cifar10, cifar100, IMDB and many others. See https://keras.io/api/datasets/ for documentation"]},{"cell_type":"code","metadata":{"id":"5j-DYkTaz3Ts","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c8d1bc5-e63c-489a-afe5-a770504c517f","executionInfo":{"status":"ok","timestamp":1653524571223,"user_tz":-330,"elapsed":404,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["import numpy as np\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"nOMU1JxB0BRH"},"source":["Mnist images are grayscale images with pixels in the range [0,255].\n","We pass to floats, and normalize them in the range [0,1]."]},{"cell_type":"code","metadata":{"id":"G78aNHyG0bWD","executionInfo":{"status":"ok","timestamp":1653524590069,"user_tz":-330,"elapsed":263,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["x_train = x_train.astype('float32') / 255.\n","x_test = x_test.astype('float32') / 255."],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5aYDPNGn0n73"},"source":["Bidimensional convolutions expect input with three dimensions (plus an additional batchsize dimension): width, height, channels. \n","Since mnist digits have only two dimensions (being in grayscale), we need to extend them with an additional dimension."]},{"cell_type":"code","metadata":{"id":"koAbxpngVCsq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a335f58c-1251-483e-daea-024d4ff0b209","executionInfo":{"status":"ok","timestamp":1653524638097,"user_tz":-330,"elapsed":232,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["(n,w,h) = x_train.shape\n","x_train = x_train.reshape(n,w,h,1)\n","(n,w,h) = x_test.shape\n","x_test = x_test.reshape(n,w,h,1)\n","print(x_train.shape)\n","print(x_test.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28, 1)\n","(10000, 28, 28, 1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"mOp2WtJJsrpn"},"source":["Mnist labels are integers in the range [0,9]. Since the network will produce probabilities for each one of these categories, if we want to compare it with the ground trouth probability using categorical crossentropy, that is the traditional choice, we should change each integer in its categorical description, using e.g. the \"to_categorical\" function in utils.\n","\n","Alternatively, we can use the so called \"sparse categorical crossentropy\" loss function https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy that allows us to directly compare predictions with labels."]},{"cell_type":"code","metadata":{"id":"ZK--l9nzs9F-","executionInfo":{"status":"ok","timestamp":1653524670353,"user_tz":-330,"elapsed":261,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["#y_train = keras.utils.to_categorical(y_train)\n","#y_test = keras.utils.to_categorical(y_test)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZO6JwhwnurFi"},"source":["Let us come to the convolutional network. We define a simple network composed by three convolutional layers, followed by a couple of Dense layers."]},{"cell_type":"code","metadata":{"id":"hVUe816fUyu6","executionInfo":{"status":"ok","timestamp":1653524814864,"user_tz":-330,"elapsed":251,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["xin = Input(shape=(28,28,1))\n","x = Conv2D(16,(3,3),strides=(2,2),padding='valid')(xin)\n","x = Conv2D(32,(3,3),strides=(2,2),padding='valid')(x)\n","x = Conv2D(64,(3,3),strides=(2,2),padding='valid')(x)\n","x = Flatten()(x)\n","x = Dense(64, activation ='relu')(x)\n","res = Dense(10,activation = 'softmax')(x)\n","\n","mynet = Model(inputs=xin,outputs=res)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wBDN-sBi7uUO"},"source":["Let's have a look at the summary"]},{"cell_type":"code","metadata":{"id":"sgWQT4jHZUR3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"369e9dc4-dcbd-40db-8d51-1b02b84eea65","executionInfo":{"status":"ok","timestamp":1653524817726,"user_tz":-330,"elapsed":408,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["mynet.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 13, 13, 16)        160       \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 6, 6, 32)          4640      \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 2, 2, 64)          18496     \n","                                                                 \n"," flatten_1 (Flatten)         (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                16448     \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 40,394\n","Trainable params: 40,394\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"iiFJ6HMQ8icY"},"source":["In valid mode, no padding is applied. \n","Along each axis, the output dimension O is computed from the input dimension I using the formula O=(I-K)/S +1, where K is the kernel dimension and S is the stride.\n","\n","For all layers, K=3 and S=2. So, for the first conv we pass from dimension 28\n","to dimension (28-3)/2+1 = 13, then to dimension (13-3)/2+1 = 6 and finally to dimension (6-3)/2+1 = 2. \n","\n","Exercise: modify \"valid\" to \"same\" and see what happens.\n","\n","The second important point is about the number of parameters.\n","You must keep in mind that a kernel of dimension K1 x K2 has an actual dimension K1 x K2 x CI, where CI is number of input channels: in other words the kernel is computing at the same time spatial and cross-channel correlations.\n","\n","So, for the first convolution, we have 3 x 3 x 1 + 1 = 10 parameters for each filter (1 for the bias), and since we are computing 16 filters, the number of parameters is 10 x 16 = 160.\n","\n","For the second convolution, each filter has 3 x 3 x 16 + 1 = 145 parameters, ans since we have 32 filters, the total number of parameters is 145 x 32 = 4640.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Lj77T0xDBbkc"},"source":["Let us come to training.\n","\n","In addition to the optimizer and the loss, we also pass a \"metrics\" argument. Metrics are additional functions that are not directly used for training, but allows us to monitor its advancement. For instance, we use accuracy, in this case (sparse, because we are using labels, and cateogrical because we have multiple categories)."]},{"cell_type":"code","metadata":{"id":"5woK9FZhd2CA","executionInfo":{"status":"ok","timestamp":1653524820920,"user_tz":-330,"elapsed":246,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["mynet.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=[metrics.SparseCategoricalAccuracy()])"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"flvXXtQwbvFR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c5d506b-8265-48bb-99c6-178e54f23058","executionInfo":{"status":"ok","timestamp":1653524905058,"user_tz":-330,"elapsed":82894,"user":{"displayName":"Shamik Basu","userId":"08062205691204303951"}}},"source":["mynet.fit(x_train,y_train, shuffle=True, epochs=10, batch_size=32,validation_data=(x_test,y_test))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1875/1875 [==============================] - 17s 3ms/step - loss: 0.2630 - sparse_categorical_accuracy: 0.9227 - val_loss: 0.1479 - val_sparse_categorical_accuracy: 0.9537\n","Epoch 2/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.1322 - sparse_categorical_accuracy: 0.9597 - val_loss: 0.1178 - val_sparse_categorical_accuracy: 0.9629\n","Epoch 3/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.1104 - sparse_categorical_accuracy: 0.9672 - val_loss: 0.1045 - val_sparse_categorical_accuracy: 0.9668\n","Epoch 4/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.0946 - sparse_categorical_accuracy: 0.9710 - val_loss: 0.1065 - val_sparse_categorical_accuracy: 0.9681\n","Epoch 5/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.0856 - sparse_categorical_accuracy: 0.9739 - val_loss: 0.0934 - val_sparse_categorical_accuracy: 0.9714\n","Epoch 6/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.0784 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.0896 - val_sparse_categorical_accuracy: 0.9727\n","Epoch 7/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.0728 - sparse_categorical_accuracy: 0.9776 - val_loss: 0.1054 - val_sparse_categorical_accuracy: 0.9675\n","Epoch 8/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.0663 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.0893 - val_sparse_categorical_accuracy: 0.9733\n","Epoch 9/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.0629 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0863 - val_sparse_categorical_accuracy: 0.9754\n","Epoch 10/10\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.0867 - val_sparse_categorical_accuracy: 0.9756\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fbbe0070590>"]},"metadata":{},"execution_count":14}]}]}